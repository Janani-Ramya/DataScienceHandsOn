# -*- coding: utf-8 -*-
"""Amazon_Recommendation_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zvqnGPK4KzcmMz4LrNWe7nQe-pn00B9V

# Exploratory Data Analysis

1.  Which movies have maximum views/ratings?
2.  What is the average rating for each movie? Define the top 5 movies with the maximum ratings.
3.  Define the top 5 movies with the least audience.
"""

import pandas as pd
import numpy as np

data = pd.read_csv('Amazon - Movies and TV Ratings.csv')

data.head(5)

data.describe().T

# Which movies have maximum views?
data.describe().T['count'].sort_values(ascending=False).head(1).to_frame()

# Which movies have maximum rating?
data.sum().drop('user_id').sort_values(ascending=False).head(1).to_frame()

# What is the average rating for each movie?
data.drop('user_id',axis=1).mean()

# Define the top 5 movies with the maximum ratings.
data.drop('user_id',axis=1).mean().sort_values(ascending=False).head(5).to_frame()

#Define the top 5 movies with the least audience.
data.describe().T['count'].sort_values(ascending=True).head(5).to_frame()

"""# Recommendation Model

1.   Divide the data into training and test data
2.   Build a recommendation model on training data
3.   Make predictions on the test data
"""

!pip install scikit-surprise

from surprise import Reader
from surprise import accuracy
from surprise import Dataset
from surprise.model_selection import train_test_split
from surprise import SVD
from surprise.model_selection import cross_validate

#Prepare data
dataset = data.melt(id_vars=data.columns[0],value_vars=data.columns[1:],var_name="Movie_id",value_name="Rating")

dataset

#Create Reader
rd = Reader(rating_scale=(-1,10))

#Load data into surprise dataset
surprise_dataset = Dataset.load_from_df(dataset.fillna(0),reader=rd)
surprise_dataset

trainset, testset = train_test_split(surprise_dataset,test_size=0.20)

svd = SVD()
svd.fit(trainset)

prediction_score = svd.test(testset)
accuracy.rmse(prediction_score)

accuracy.mae(prediction_score)

#Test with single known data
svd.predict('A2647CKYEBQE7N', 'Movie12', r_ui=5.0, verbose= True)

#Perform Cross validation
cross_validate(svd, surprise_dataset, measures = ['RMSE', 'MAE'], cv = 3, verbose = True)

def validate(dframe,min_,max_):
    svd = SVD()
    rd = Reader(rating_scale=(-1,10))
    data = Dataset.load_from_df(dframe,reader=rd)
    print(cross_validate(svd, data, measures = ['RMSE', 'MAE'], cv = 3, verbose = True))
    print("**"*10)
    u_id = 'A2647CKYEBQE7N'
    m_id = 'Movie12'
    ra_u = 5.0
    print(svd.predict(u_id,m_id,r_ui=ra_u,verbose=True))
    print("**"*10)
    print()

validate(dataset.fillna(0),-1,10)

validate(dataset.fillna(dataset.mean()),-1,10)

validate(dataset.fillna(dataset.median()),-1,10)

#trying grid search and find optimum hyperparameter value for n_factors
from surprise.model_selection import GridSearchCV

param_grid = {'n_epochs':[20,30],
             'lr_all':[0.005,0.001],
             'n_factors':[50,100]}

gs = GridSearchCV(SVD,param_grid,measures=['rmse','mae'],cv=3)
gs.fit(surprise_dataset)

gs.best_score

print(gs.best_score["rmse"])
print(gs.best_params["rmse"])
